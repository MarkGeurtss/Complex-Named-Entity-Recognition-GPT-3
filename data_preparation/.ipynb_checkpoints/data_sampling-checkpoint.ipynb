{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd43a16",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "The test data provided by the MultiCoNER II shared task contains a large number of data instances. Due to the expenses linked with running GPT-3, we are unable to apply the model to the entire data set. In this file we randomly sample instances from the test data as test set, as well as from the validation file to be used as few-shot sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data file\n",
    "# It should be loaded in as a DataFrame containing the columns \"id\" and \"domain\"\n",
    "# where a row contains a token in the column \"id\" and the corresponding BIO scheme\n",
    "# tag in column \"domain\"\n",
    "\n",
    "test_full = pd.read_csv(\"...\")\n",
    "test_full = pd.DataFrame(test_full)\n",
    "\n",
    "print(test_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data file\n",
    "# It should be loaded in as a DataFrame containing the columns \"id\" and \"domain\"\n",
    "# where a row contains a token in the column \"id\" and the corresponding BIO scheme\n",
    "# tag in column \"domain\"\n",
    "\n",
    "dev = pd.read_csv(\"...\")\n",
    "dev = pd.DataFrame(dev)\n",
    "\n",
    "print(dev.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_gpt_output(token_df):\n",
    "    sentences = []\n",
    "    current_sentence = ''\n",
    "    curr_tag = None\n",
    "    curr_tokens = []\n",
    "    tag_tokens = []\n",
    "    ne_list = []\n",
    "    sent_tags = []\n",
    "    tags = []\n",
    "    \n",
    "\n",
    "    # iterate over the rows of the dataframe\n",
    "    for i, (word, tag) in enumerate(zip(token_df['id'], token_df['domain'])):\n",
    "        if str(word).startswith(\"# id\"):\n",
    "            # Append previous sentence\n",
    "            sentences.append(current_sentence.strip())\n",
    "            if curr_tokens:\n",
    "                tag_tokens.append(str(curr_tag + ' (' + ' '.join(curr_tokens) + ')'))\n",
    "            \n",
    "            ne_list.append(tag_tokens)\n",
    "            curr_tokens = []\n",
    "            curr_tag = None\n",
    "            tags.append(sent_tags)\n",
    "            \n",
    "            # Reset current sentence\n",
    "            current_sentence = ''\n",
    "            sent_tags = []\n",
    "            tag_tokens = []\n",
    "            \n",
    "        else: \n",
    "            # add the current word to the current sentence\n",
    "            current_sentence += str(word) + \" \"\n",
    "            if str(tag).startswith('B-'):\n",
    "                if curr_tokens:\n",
    "                    tag_tokens.append(curr_tag + ' (' + ' '.join(curr_tokens) + ')')\n",
    "                curr_tokens = [str(word)]\n",
    "                curr_tag = tag[2:]\n",
    "                sent_tags.append(tag[2:])\n",
    "                \n",
    "            elif str(tag).startswith('I-'):\n",
    "                curr_tokens.append(str(word))\n",
    "                sent_tags.append(tag[2:])\n",
    "                \n",
    "            else:\n",
    "                if curr_tokens:\n",
    "                    tag_tokens.append(curr_tag + ' (' + ' '.join(curr_tokens) + ')')\n",
    "                curr_tokens = []\n",
    "                sent_tags.append(tag)\n",
    "\n",
    "    # add the last sentence to the list of sentences\n",
    "    sentences.append(current_sentence.strip())\n",
    "    tags.append(sent_tags)\n",
    "    ne_list.append(tag_tokens)\n",
    "    \n",
    "#     print(sentences)\n",
    "#     print('-' * 25)\n",
    "#     print(tags)\n",
    "#     print('-' * 25)\n",
    "#     print(ne_list)\n",
    "    \n",
    "\n",
    "    sentence_df = pd.DataFrame({'sentence': sentences, 'tags': tags, 'fewshot_gpt': ne_list})\n",
    "    return sentence_df\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_xlm_output(token_df):\n",
    "    sentences = []\n",
    "    current_sentence = ''\n",
    "    curr_tag = None\n",
    "    curr_tokens = []\n",
    "    tag_tokens = []\n",
    "    ne_list = []\n",
    "    sent_tags = []\n",
    "    tags = []\n",
    "    \n",
    "\n",
    "    # iterate over the rows of the dataframe\n",
    "    for i, (word, tag) in enumerate(zip(token_df['id'], token_df['domain'])):\n",
    "        if str(word).startswith(\"# id\"):\n",
    "            # Append previous sentence\n",
    "            sentences.append(current_sentence.strip())\n",
    "            if curr_tokens:\n",
    "                tag_tokens.append(str(curr_tag + ' (' + ' '.join(curr_tokens) + ')'))\n",
    "            \n",
    "            ne_list.append(tag_tokens)\n",
    "            curr_tokens = []\n",
    "            curr_tag = None\n",
    "            tags.append(sent_tags)\n",
    "            \n",
    "            # Reset current sentence\n",
    "            current_sentence = ''\n",
    "            sent_tags = []\n",
    "            tag_tokens = []\n",
    "            \n",
    "        else: \n",
    "            # add the current word to the current sentence\n",
    "            current_sentence += str(word) + \" \"\n",
    "            if str(tag).startswith('B-'):\n",
    "                if curr_tokens:\n",
    "                    tag_tokens.append(curr_tag + ' (' + ' '.join(curr_tokens) + ')')\n",
    "                curr_tokens = [str(word)]\n",
    "                curr_tag = tag[2:]\n",
    "                sent_tags.append(tag)\n",
    "                \n",
    "            elif str(tag).startswith('I-'):\n",
    "                curr_tokens.append(str(word))\n",
    "                sent_tags.append(tag)\n",
    "                \n",
    "            else:\n",
    "                if curr_tokens:\n",
    "                    tag_tokens.append(curr_tag + ' (' + ' '.join(curr_tokens) + ')')\n",
    "                curr_tokens = []\n",
    "                sent_tags.append(tag)\n",
    "\n",
    "    # add the last sentence to the list of sentences\n",
    "    sentences.append(current_sentence.strip())\n",
    "    tags.append(sent_tags)\n",
    "    ne_list.append(tag_tokens)\n",
    "    \n",
    "#     print(sentences)\n",
    "#     print('-' * 25)\n",
    "#     print(tags)\n",
    "#     print('-' * 25)\n",
    "#     print(ne_list)\n",
    "    \n",
    "\n",
    "    sentence_df = pd.DataFrame({'sentence': sentences, 'tags': tags, 'fewshot_gpt': ne_list})\n",
    "    return sentence_df\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt = sentence_gpt_output(test_full)\n",
    "test_gpt.to_csv('...', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2813e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_4000_1 = test_gpt.sample(n=4000, replace=False)\n",
    "test_gpt_4000_1.to_csv('...', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_gpt = sentence_gpt_output(dev)\n",
    "dev_gpt.to_csv('...', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_gpt_10 = dev_gpt.sample(n=10, replace=False)\n",
    "dev_gpt_10.to_csv('...', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e067055",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_gpt_10_2 = dev_gpt.sample(n=10, replace=False)\n",
    "dev_gpt_10_2.to_csv('...', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xlm = sentence_xlm_output(test_full)\n",
    "test_xlm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba585173",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xlm_4000_1 = test_xlm[test_xlm['sentence'].isin(test_gpt_4000_1['sentence'])]\n",
    "test_xlm_4000_1 = test_xlm_4000_1.drop_duplicates(subset=['sentence'])\n",
    "test_xlm_4000_1.to_csv('...', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
