{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an MMLU Eval\n",
    "\n",
    "This notebook shows how to:\n",
    "- Build and run an eval\n",
    "- Load the results and into a Pandas Dataframe\n",
    "\n",
    "We use the `evals.elsuite.basic.match:Match` Eval class here to check whether new completions match the correct answer. Under the hood, it will generate a completion with the choice of model for each prompt, check if the completion matches the true answer, then logs a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install, and download MMLU if you haven't already\n",
    "#%pip install -e .\n",
    "\n",
    "!curl -O https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
    "!tar -xf data.tar\n",
    "data_pth = \"../../../multiconer2023/EN-English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Assuming this notebook is in examples/\n",
    "registry_pth = os.path.join(os.getcwd(), \"../evals/registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for my attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prompts using Chat format. We support converting Chat conversations to text for non-Chat models\n",
    "\n",
    "sys_msg = \"\"\"Fill in the prompt you want to use to send the input to GPT-3\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_chat_prompt(sys_msg, sentence):\n",
    "    user_prompt = f\"\\n{sentence}\" + \"\\nAnswer:\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_msg}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "def create_chat_example(sentence, correct_answer):\n",
    "    \"\"\"\n",
    "    Form few-shot prompts in the recommended format: https://github.com/openai/openai-python/blob/main/chatml.md#few-shot-prompting\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\\n{sentence}\" + \"\\nAnswer:\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": user_prompt, \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": correct_answer, \"name\": \"example_assistant\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load in the test data and the few-shot dataset\n",
    "test_df = pd.read_csv('...')\n",
    "dev_df = pd.read_csv('...')\n",
    "\n",
    "registry_yaml = {}\n",
    "\n",
    "# Create few-shot prompts\n",
    "dev_df['sample'] = dev_df.apply(lambda x: create_chat_example(x['sentence'], x['fewshot_gpt']), axis=1)\n",
    "few_shot_pth = os.path.join(registry_pth, 'data', 'thesis_test', \"few_shot.jsonl\") \n",
    "dev_df[[\"sample\"]].to_json(few_shot_pth, lines=True, orient=\"records\")\n",
    "\n",
    "# Create test prompts and ideal completions\n",
    "test_df['input'] = test_df.apply(lambda x: create_chat_prompt(sys_msg, x['sentence']), axis=1)\n",
    "test_df['ideal'] = test_df.tags\n",
    "samples_pth = os.path.join(registry_pth, 'data', 'thesis_test', \"samples.jsonl\")     \n",
    "test_df[[\"input\", \"ideal\"]].to_json(samples_pth, lines=True, orient=\"records\")\n",
    "\n",
    "eval_id = \"match_mmlu_thesis\"\n",
    "\n",
    "registry_yaml[eval_id] = {\n",
    "    \"id\": f\"{eval_id}.test.v1\",\n",
    "    \"metrics\": ['accuracy']\n",
    "}\n",
    "\n",
    "# Adjust the \"num_few_shot\" parameter to the number of few-shot samples you load in\n",
    "registry_yaml[f\"{eval_id}.test.v1\"] = {\n",
    "    \"class\": \"evals.elsuite.basic.match:Match\",\n",
    "    \"args\": {\n",
    "        \"samples_jsonl\": samples_pth,\n",
    "        \"few_shot_jsonl\": few_shot_pth,\n",
    "        \"num_few_shot\": 10,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(registry_pth, \"evals\", \"thesis.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will generate a JSONL which will record samples and logs and store it in /tmp/evallogs\n",
    "!oaieval gpt-3.5-turbo match_mmlu_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to process the log events generated by oaieval\n",
    "# assign the path to the json file with the models outputs to the variable \"events\"\n",
    "events = \"...\"\n",
    "\n",
    "with open(events, \"r\", encoding='utf-8') as f:\n",
    "    events_df = pd.read_json(f, lines=True, encoding='utf-8')\n",
    "    \n",
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect samples\n",
    "for i, r in pd.json_normalize(events_df[events_df.type == \"sampling\"].data).iterrows():\n",
    "    print(f\"Prompt: {r.prompt}\")\n",
    "    print(f\"Sampled: {r.sampled}\")\n",
    "    print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df[~events_df['sample_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import index_natsorted\n",
    "\n",
    "events_df['order'] = events_df['sample_id'].apply(lambda x: x[23:])\n",
    "events_df['order'] = events_df['order'].astype(int)\n",
    "\n",
    "# get the index of the sorted values using natural sort\n",
    "index = index_natsorted(events_df['order'])\n",
    "events_df = events_df.iloc[index]\n",
    "events_df = events_df[events_df.type == \"sampling\"]\n",
    "events_df.drop_duplicates(subset='order', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "\n",
    "for i, r in pd.json_normalize(events_df.data).iterrows():\n",
    "    predictions.append(r.sampled[0])\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions, columns=['predictions'])\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.concat([test_df, predictions_df], axis=1)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong_output(df):\n",
    "    counter = 0\n",
    "    no_entities_counter = 0\n",
    "    more_context_counter = 0\n",
    "    typo_counter = 0\n",
    "    no_entities_from_list_counter = 0\n",
    "    language_counter = 0\n",
    "    policy_counter = 0\n",
    "    no_entities = ['There are no named entities in this sentence.', 'does not contain any recognizable entities']\n",
    "    more_context = ['context', 'more information', 'incomplete']\n",
    "    typo = ['typo', 'spelling mistake', 'not grammatically correct', 'error']\n",
    "    no_entities_from_list = ['match the given tags', 'from the list provided', 'match the specified tags', 'named entities from the given list']\n",
    "    language = ['English']\n",
    "    policy = [\"against OpenAI's use case policy\"]\n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            eval(df['predictions'][i])\n",
    "        \n",
    "        except:\n",
    "            counter += 1\n",
    "            print('-' * 30, '\\n',\n",
    "                  df['sentence'][i], '\\n',\n",
    "                  df['predictions'][i], '\\n',\n",
    "                  '-' * 30)\n",
    "            for string in no_entities:\n",
    "                if string in df['predictions'][i]:\n",
    "                    no_entities_counter += 1\n",
    "                    break\n",
    "            for string in more_context:\n",
    "                if string in df['predictions'][i]:\n",
    "                    more_context_counter += 1\n",
    "                    break\n",
    "            for string in typo:\n",
    "                if string in df['predictions'][i]:\n",
    "                    typo_counter += 1\n",
    "                    break\n",
    "            for string in no_entities_from_list:\n",
    "                if string in df['predictions'][i]:\n",
    "                    no_entities_from_list_counter += 1\n",
    "                    break\n",
    "            for string in language:\n",
    "                if string in df['predictions'][i]:\n",
    "                    language_counter += 1\n",
    "                    break\n",
    "            for string in policy:\n",
    "                if string in df['predictions'][i]:\n",
    "                    policy_counter += 1\n",
    "                    break\n",
    "            index_list.append(i)\n",
    "                    \n",
    "    print('There were', counter, 'wrong outputs')\n",
    "    print(no_entities_from_list_counter, 'were due to there not being entities in the sentence that corresponded to a label in the provided list')\n",
    "    print(no_entities_counter, 'were due to GPT-3 stating there were no NEs in the provided sentence')\n",
    "    print(more_context_counter, 'were due to GPT-3 requesting more context')\n",
    "    print(typo_counter, 'were due to GPT-3 stating there was a spelling or grammar mistake in the sentence')\n",
    "    print(language_counter, 'were due to the sentence provided not being English')\n",
    "    \n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_index_list = wrong_output(comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def allign_predictions(df):\n",
    "    prediction_tags = []\n",
    "    gold_tags = []\n",
    "    \n",
    "    actual_tags = ['Facility', 'OtherLOC', 'HumanSettlement', 'Station', \n",
    "                   'VisualWork', 'MusicalWork', 'WrittenWork', 'ArtWork', \n",
    "                   'Software', 'MusicalGRP', 'PublicCorp', 'PrivateCorp', \n",
    "                   'AerospaceManufacturer', 'SportsGRP', 'CarManufacturer', \n",
    "                   'ORG', 'Scientist', 'Artist', 'Athlete', 'Politician', \n",
    "                   'Cleric', 'SportsManager', 'OtherPER', 'Clothing', \n",
    "                   'Vehicle', 'Food', 'Drink', 'OtherPROD', 'Medication/Vaccine', \n",
    "                   'MedicalProcedure', 'AnatomicalStructure', 'Symptom', 'Disease']\n",
    "    \n",
    "    elab_tags = ['Facility', 'OtherLocation', 'HumanSettlement', 'Station', \n",
    "                 'VisualWork', 'MusicalWork', 'WrittenWork', 'ArtWork', 'Software', \n",
    "                 'MusicalGroup', 'PublicCorporation', 'PrivateCorporation', \n",
    "                 'AerospaceManufacturer', 'SportsGroup', 'CarManufacturer', \n",
    "                 'Organization', 'Scientist', 'Artist', 'Athlete', 'Politician', \n",
    "                 'Cleric', 'SportsManager', 'OtherPerson', 'Clothing',\n",
    "                 'Vehicle', 'Food', 'Drink', 'OtherProduct', 'Medication/Vaccine', \n",
    "                 'MedicalProcedure', 'AnatomicalStructure', 'Symptom', 'Disease']\n",
    "    \n",
    "    normal_counter = 0\n",
    "    group_counter = 0\n",
    "    except_counter = 0\n",
    "    continue_counter = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tag_list = []\n",
    "        previous_words = []\n",
    "\n",
    "        for word in row['sentence'].split(' '):\n",
    "            track = False\n",
    "            previous_words.append(word)\n",
    "            pred_counter = 0\n",
    "            pred_entities = []\n",
    "            try:\n",
    "                for prediction in eval(row['predictions']):\n",
    "                    entity_tokens = []\n",
    "                    pred_counter += 1\n",
    "                    normal_counter += 1\n",
    "                    groups = re.match('^([^ ]+)(?:\\s\\((.+)\\))?$', prediction)\n",
    "                    named_entities = groups.group(2).split(', ')\n",
    "                    predicted_tag = groups.group(1)\n",
    "                    \n",
    "                    for named_entity in named_entities:\n",
    "                        entity_tokens.extend(named_entity.split(' '))\n",
    "                        pred_entities.extend(named_entity.split(' '))\n",
    "                    \n",
    "                    group_counter += 1\n",
    "                    if word in entity_tokens:\n",
    "                        if (predicted_tag in elab_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_index = elab_tags.index(predicted_tag)\n",
    "                            tag_list.append(str(actual_tags[tag_index]))\n",
    "                            track = True\n",
    "            except:\n",
    "                except_counter += 1\n",
    "                pass\n",
    "            \n",
    "            if track == False:\n",
    "                tag_list.append('O')\n",
    "        prediction_tags.append(tag_list)\n",
    "        \n",
    "        # Cast the strings in tags to actual lists\n",
    "        gold_tags.append(eval(row['tags']))\n",
    "        \n",
    "    final_df = pd.DataFrame({'sentence': df['sentence'], 'tags': gold_tags, \n",
    "                             'prediction': df['predictions'], 'prediction_list': prediction_tags})\n",
    "    \n",
    "    print(normal_counter, group_counter, except_counter)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = allign_predictions(comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_increment_key(dictionary, key):\n",
    "    dictionary[key] = dictionary.setdefault(key, 0) + 1\n",
    "\n",
    "def check_correctness_wrongs(dataframe):\n",
    "    label_recog_err = {}\n",
    "    label_count = {}\n",
    "    label_frac = {}\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        previous_label = ''\n",
    "        gold_tags = dataframe.iloc[index]['tags']\n",
    "        pred_tags = dataframe.iloc[index]['prediction_list']\n",
    "        for ind in range(len(gold_tags)):\n",
    "            if gold_tags[ind] != 'O':\n",
    "                add_or_increment_key(label_count, gold_tags[ind])\n",
    "            if gold_tags[ind] != 'O' and pred_tags[ind] == 'O':\n",
    "                add_or_increment_key(label_recog_err, gold_tags[ind])\n",
    "                \n",
    "    for label in label_count.keys():\n",
    "        label_frac[label] = label_recog_err[label] / label_count[label]\n",
    "\n",
    "    sorted_dict = dict(sorted(label_frac.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    for label in sorted_dict.keys():\n",
    "        print(label, ' ', label_count[label], ' ', label_recog_err[label], ' ', \"{:.3f}\".format(label_recog_err[label] / label_count[label]))\n",
    "\n",
    "\n",
    "check_correctness_wrongs(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_or_increment_key(dictionary, key):\n",
    "    dictionary[key] = dictionary.setdefault(key, 0) + 1\n",
    "\n",
    "def check_correctness_wrongs(dataframe):\n",
    "    label_recog_err = {}\n",
    "    label_count = {}\n",
    "    label_frac = {}\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        previous_label = ''\n",
    "        tracker = False\n",
    "        gold_tags = dataframe.iloc[index]['tags']\n",
    "        pred_tags = dataframe.iloc[index]['prediction_list']\n",
    "        for ind in range(len(gold_tags)):\n",
    "            if gold_tags[ind] == 'O':\n",
    "                previous_label = ''\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != previous_label:\n",
    "                tracker = False\n",
    "                previous_label = gold_tags[ind]\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != pred_tags[ind]:\n",
    "                tracker = True\n",
    "                            \n",
    "            try:\n",
    "                next_label = gold_tags[ind + 1]\n",
    "            except:\n",
    "                next_label = ''\n",
    "                \n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != next_label:\n",
    "                add_or_increment_key(label_count, gold_tags[ind])\n",
    "            if gold_tags[ind] != next_label and tracker:\n",
    "                add_or_increment_key(label_recog_err, gold_tags[ind])\n",
    "                tracker = False\n",
    "                \n",
    "    for label in label_count.keys():\n",
    "        label_frac[label] = label_recog_err[label] / label_count[label]\n",
    "\n",
    "    sorted_dict = dict(sorted(label_frac.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    for label in sorted_dict.keys():\n",
    "        print(label, ' ', label_count[label], ' ', label_recog_err[label], ' ', \"{:.3f}\".format(label_recog_err[label] / label_count[label]))\n",
    "\n",
    "\n",
    "check_correctness_wrongs(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Create a set of unique tags\n",
    "unique_tags = set().union(*final_df['tags'], *final_df['prediction_list'])\n",
    "\n",
    "# Fit the LabelEncoder on the unique tags\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(list(unique_tags))\n",
    "\n",
    "# Convert tags to numerical labels\n",
    "final_df['tags_encoded'] = final_df['tags'].apply(lambda x: encoder.transform(x))\n",
    "final_df['prediction_encoded'] = final_df['prediction_list'].apply(lambda x: encoder.transform(x))\n",
    "\n",
    "# Flatten encoded tag sequences\n",
    "tags_flat = [tag for sublist in final_df['tags_encoded'] for tag in sublist]\n",
    "prediction_flat = [tag for sublist in final_df['prediction_encoded'] for tag in sublist]\n",
    "\n",
    "for index in range(len(tags_flat) - 1, -1, -1):\n",
    "    if tags_flat[index] == 17:\n",
    "        del tags_flat[index]\n",
    "        del prediction_flat[index]\n",
    "\n",
    "# Calculate macro F1 score\n",
    "f1_macro = f1_score(tags_flat, prediction_flat, average='macro', zero_division = 1)\n",
    "f1_weighted = f1_score(tags_flat, prediction_flat, average='weighted', zero_division = 1)\n",
    "\n",
    "# Convert back to original tag labels\n",
    "tags_original = encoder.inverse_transform(tags_flat)\n",
    "prediction_original = encoder.inverse_transform(prediction_flat)\n",
    "\n",
    "# Calculate per-label metrics\n",
    "classification_rep = classification_report(tags_original, prediction_original, zero_division = 1)\n",
    "\n",
    "print(\"Macro F1 Score:\", f1_macro)\n",
    "print('Weighted F1 Score:', f1_weighted)\n",
    "print(\"Per-label Metrics:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def allign_predictions_class(df):\n",
    "    prediction_tags = []\n",
    "    gold_tags = []\n",
    "\n",
    "    org_loc_tags = ['Facility', 'OtherLOC', 'HumanSettlement', 'Station']\n",
    "    org_cw_tags = ['VisualWork', 'MusicalWork', 'WrittenWork', 'ArtWork', 'Software']\n",
    "    org_grp_tags = ['MusicalGRP', 'PublicCorp', 'PrivateCorp', 'AerospaceManufacturer', \n",
    "                    'SportsGRP', 'CarManufacturer', 'ORG']\n",
    "    org_prs_tags = ['Scientist', 'Artist', 'Athlete', 'Politician', 'Cleric', 'SportsManager', 'OtherPER']\n",
    "    org_prod_tags = ['Clothing', 'Vehicle', 'Food', 'Drink', 'OtherPROD']\n",
    "    org_med_tags = ['Medication/Vaccine', 'MedicalProcedure', 'AnatomicalStructure', 'Symptom', 'Disease']\n",
    "\n",
    "    loc_tags = ['Facility', 'OtherLocation', 'HumanSettlement', 'Station']\n",
    "    cw_tags = ['VisualWork', 'MusicalWork', 'WrittenWork', 'ArtWork', 'Software']\n",
    "    grp_tags = ['MusicalGroup', 'PublicCorporation', 'PrivateCorporation', \n",
    "                 'AerospaceManufacturer', 'SportsGroup', 'CarManufacturer', \n",
    "                 'Organization']\n",
    "    prs_tags = ['Scientist', 'Artist', 'Athlete', 'Politician', \n",
    "                 'Cleric', 'SportsManager', 'OtherPerson']\n",
    "    prod_tags = ['Clothing', 'Vehicle', 'Food', 'Drink', 'OtherProduct']\n",
    "    med_tags = ['Medication/Vaccine', 'MedicalProcedure', 'AnatomicalStructure', 'Symptom', 'Disease']\n",
    "    \n",
    "    normal_counter = 0\n",
    "    group_counter = 0\n",
    "    except_counter = 0\n",
    "    continue_counter = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tag_list = []\n",
    "        previous_words = []\n",
    "        gold_list = []\n",
    "\n",
    "        for word in row['sentence'].split(' '):\n",
    "            track = False\n",
    "            previous_words.append(word)\n",
    "            pred_counter = 0\n",
    "            pred_entities = []\n",
    "            try:\n",
    "                for prediction in eval(row['predictions']):\n",
    "                    entity_tokens = []\n",
    "                    pred_counter += 1\n",
    "                    normal_counter += 1\n",
    "                    groups = re.match('^([^ ]+)(?:\\s\\((.+)\\))?$', prediction)\n",
    "                    named_entities = groups.group(2).split(', ')\n",
    "                    predicted_tag = groups.group(1)\n",
    "                    \n",
    "                    for named_entity in named_entities:\n",
    "                        entity_tokens.extend(named_entity.split(' '))\n",
    "                        pred_entities.extend(named_entity.split(' '))\n",
    "                    \n",
    "                    group_counter += 1\n",
    "                    if word in entity_tokens:\n",
    "                        if (predicted_tag in loc_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Location')\n",
    "                            track = True\n",
    "                        elif (predicted_tag in cw_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Creative Work')\n",
    "                            track = True\n",
    "                        elif (predicted_tag in grp_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Group')\n",
    "                            track = True\n",
    "                        elif (predicted_tag in prs_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Person')\n",
    "                            track = True\n",
    "                        elif (predicted_tag in prod_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Product')\n",
    "                            track = True\n",
    "                        elif (predicted_tag in med_tags) and (track == False) and (previous_words.count(word) == pred_entities.count(word)):\n",
    "                            tag_list.append('Medical')\n",
    "                            track = True\n",
    "                        \n",
    "            except:\n",
    "                except_counter += 1\n",
    "                pass\n",
    "            \n",
    "            if track == False:\n",
    "                tag_list.append('O')\n",
    "        prediction_tags.append(tag_list)\n",
    "        \n",
    "        for tag in eval(row['tags']):\n",
    "            if tag == 'O':\n",
    "                gold_list.append(tag)\n",
    "            elif tag in org_loc_tags:\n",
    "                gold_list.append('Location')\n",
    "            elif tag in org_cw_tags:\n",
    "                gold_list.append('Creative Work')\n",
    "            elif tag in org_grp_tags:\n",
    "                gold_list.append('Group')\n",
    "            elif tag in org_prs_tags:\n",
    "                gold_list.append('Person')\n",
    "            elif tag in org_prod_tags:\n",
    "                gold_list.append('Product')\n",
    "            elif tag in org_med_tags:\n",
    "                gold_list.append('Medical')\n",
    "            \n",
    "            \n",
    "        # Cast the strings in tags to actual lists\n",
    "        gold_tags.append(gold_list)\n",
    "        \n",
    "    final_df = pd.DataFrame({'sentence': df['sentence'], 'tags': gold_tags, \n",
    "                             'prediction': df['predictions'], 'prediction_list': prediction_tags})\n",
    "    \n",
    "    print(normal_counter, group_counter, except_counter)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_final_df = allign_predictions_class(comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Create a set of unique tags\n",
    "unique_tags = set().union(*class_final_df['tags'], *class_final_df['prediction_list'])\n",
    "\n",
    "# Fit the LabelEncoder on the unique tags\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(list(unique_tags))\n",
    "\n",
    "# Convert tags to numerical labels\n",
    "class_final_df['tags_encoded'] = class_final_df['tags'].apply(lambda x: encoder.transform(x))\n",
    "class_final_df['prediction_encoded'] = class_final_df['prediction_list'].apply(lambda x: encoder.transform(x))\n",
    "\n",
    "# Flatten encoded tag sequences\n",
    "tags_flat = [tag for sublist in class_final_df['tags_encoded'] for tag in sublist]\n",
    "prediction_flat = [tag for sublist in class_final_df['prediction_encoded'] for tag in sublist]\n",
    "\n",
    "for index in range(len(tags_flat) - 1, -1, -1):\n",
    "    if tags_flat[index] == 4:\n",
    "        del tags_flat[index]\n",
    "        del prediction_flat[index]\n",
    "\n",
    "# Calculate macro F1 score\n",
    "f1_macro = f1_score(tags_flat, prediction_flat, average='macro')\n",
    "f1_weighted = f1_score(tags_flat, prediction_flat, average='weighted')\n",
    "\n",
    "# Convert back to original tag labels\n",
    "tags_original = encoder.inverse_transform(tags_flat)\n",
    "prediction_original = encoder.inverse_transform(prediction_flat)\n",
    "\n",
    "# Calculate per-label metrics\n",
    "classification_rep = classification_report(tags_original, prediction_original)\n",
    "\n",
    "print(\"Class Macro F1 Score:\", f1_macro)\n",
    "print(\"Class Weighted F1 Score:\", f1_weighted)\n",
    "print(\"Per-label Metrics:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_correctness_wrongs(class_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_wrong_predictions_no_o(dataframe):\n",
    "    label_count = {}\n",
    "    wrong_class_count = {}\n",
    "    wrong_count = {}\n",
    "    label_frac = {}\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        previous_label = ''\n",
    "        tracker = False\n",
    "        wrong_labels = []\n",
    "        gold_tags = dataframe.iloc[index]['tags']\n",
    "        pred_tags = dataframe.iloc[index]['prediction_list']\n",
    "        for ind in range(len(gold_tags)):\n",
    "            if gold_tags[ind] == 'O':\n",
    "                previous_label = ''\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != previous_label:\n",
    "                tracker = False\n",
    "                previous_label = gold_tags[ind]\n",
    "                wrong_labels = []\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != pred_tags[ind]:\n",
    "                tracker = True\n",
    "                if pred_tags[ind] != 'O':\n",
    "                    if pred_tags[ind] not in wrong_labels:\n",
    "                        wrong_labels.append(pred_tags[ind])\n",
    "                \n",
    "                \n",
    "                            \n",
    "            try:\n",
    "                next_label = gold_tags[ind + 1]\n",
    "            except:\n",
    "                next_label = ''\n",
    "                \n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != next_label:\n",
    "                add_or_increment_key(label_count, gold_tags[ind])\n",
    "            if gold_tags[ind] != next_label and tracker:\n",
    "                add_or_increment_key(wrong_count, gold_tags[ind])\n",
    "                for label in wrong_labels:\n",
    "                    if gold_tags[ind] not in wrong_class_count:\n",
    "                        wrong_class_count[gold_tags[ind]] = {}\n",
    "                    if label not in wrong_class_count[gold_tags[ind]]:\n",
    "                        wrong_class_count[gold_tags[ind]][label] = 0\n",
    "                    wrong_class_count[gold_tags[ind]][label] += 1  # Increment count for the predicted class\n",
    "                tracker = False\n",
    "                wrong_labels = []\n",
    "                \n",
    "    for label in label_count.keys():\n",
    "        label_frac[label] = wrong_count[label] / label_count[label]\n",
    "\n",
    "    sorted_dict = dict(sorted(label_frac.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    common_wrong = {}\n",
    "    \n",
    "    for gold in label_count.keys():\n",
    "        inner_dict = wrong_class_count[gold]\n",
    "        sorted_items = sorted(inner_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "        max_key, max_value = sorted_items[0]\n",
    "        try:\n",
    "            second_max_key, second_max_value = sorted_items[1]\n",
    "        except IndexError:\n",
    "            second_max_key, second_max_value = 'none', 0\n",
    "        common_wrong[gold] = {max_key: max_value, second_max_key: second_max_value}\n",
    "    \n",
    "\n",
    "    for label in sorted_dict.keys():\n",
    "        print(label, \" \", label_count[label], \" \", list(common_wrong[label].keys())[0], \" \", list(common_wrong[label].values())[0], \" \", list(common_wrong[label].keys())[1], \" \", list(common_wrong[label].values())[1])\n",
    "\n",
    "find_most_frequent_wrong_predictions_no_o(final_df)\n",
    "check_correctness_wrongs(class_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_wrong_predictions(dataframe):\n",
    "    label_count = {}\n",
    "    wrong_class_count = {}\n",
    "    wrong_count = {}\n",
    "    label_frac = {}\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        previous_label = ''\n",
    "        tracker = False\n",
    "        wrong_labels = []\n",
    "        gold_tags = dataframe.iloc[index]['tags']\n",
    "        pred_tags = dataframe.iloc[index]['prediction_list']\n",
    "        for ind in range(len(gold_tags)):\n",
    "            if gold_tags[ind] == 'O':\n",
    "                previous_label = ''\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != previous_label:\n",
    "                tracker = False\n",
    "                previous_label = gold_tags[ind]\n",
    "                wrong_labels = []\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != pred_tags[ind]:\n",
    "                tracker = True\n",
    "                if pred_tags[ind] not in wrong_labels:\n",
    "                    wrong_labels.append(pred_tags[ind])\n",
    "                \n",
    "                \n",
    "                            \n",
    "            try:\n",
    "                next_label = gold_tags[ind + 1]\n",
    "            except:\n",
    "                next_label = ''\n",
    "                \n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != next_label:\n",
    "                add_or_increment_key(label_count, gold_tags[ind])\n",
    "            if gold_tags[ind] != next_label and tracker:\n",
    "                add_or_increment_key(wrong_count, gold_tags[ind])\n",
    "                for label in wrong_labels:\n",
    "                    if gold_tags[ind] not in wrong_class_count:\n",
    "                        wrong_class_count[gold_tags[ind]] = {}\n",
    "                    if label not in wrong_class_count[gold_tags[ind]]:\n",
    "                        wrong_class_count[gold_tags[ind]][label] = 0\n",
    "                    wrong_class_count[gold_tags[ind]][label] += 1  # Increment count for the predicted class\n",
    "                tracker = False\n",
    "                wrong_labels = []\n",
    "                \n",
    "    for label in label_count.keys():\n",
    "        label_frac[label] = wrong_count[label] / label_count[label]\n",
    "\n",
    "    sorted_dict = dict(sorted(label_frac.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    common_wrong = {}\n",
    "    \n",
    "    for gold in label_count.keys():\n",
    "        inner_dict = wrong_class_count[gold]\n",
    "        sorted_items = sorted(inner_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "        max_key, max_value = sorted_items[0]\n",
    "        try:\n",
    "            second_max_key, second_max_value = sorted_items[1]\n",
    "        except IndexError:\n",
    "            second_max_key, second_max_value = 'none', 0\n",
    "        common_wrong[gold] = {max_key: max_value, second_max_key: second_max_value}\n",
    "    \n",
    "    for label in sorted_dict.keys():\n",
    "        print(label, \" \", label_count[label], \" \", list(common_wrong[label].keys())[0], \" \", list(common_wrong[label].values())[0], \" \", list(common_wrong[label].keys())[1], \" \", list(common_wrong[label].values())[1])\n",
    "\n",
    "find_most_frequent_wrong_predictions(final_df)\n",
    "check_correctness_wrongs(class_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_wrong_predictions_no_o(gold_labels, predictions):\n",
    "    class_counts = {}  # Dictionary to store counts of wrong predictions for each class\n",
    "    gold_label_counts = {}  # Dictionary to store counts of gold label occurrences\n",
    "    wrong_count = {}\n",
    "\n",
    "    for gold, pred in zip(gold_labels, predictions):\n",
    "        if gold not in gold_label_counts:\n",
    "            gold_label_counts[gold] = 0\n",
    "        gold_label_counts[gold] += 1  # Increment count for gold label occurrence\n",
    "\n",
    "        if gold != pred:  # Check if the prediction is wrong\n",
    "            add_or_increment_key(wrong_count, gold)\n",
    "            if gold not in class_counts:\n",
    "                class_counts[gold] = {}\n",
    "            if pred not in class_counts[gold] and pred != 'O':\n",
    "                class_counts[gold][pred] = 0\n",
    "            if pred != 'O':\n",
    "                class_counts[gold][pred] += 1  # Increment count for the predicted class\n",
    "\n",
    "    most_frequent_wrong_predictions = {}\n",
    "    for gold in class_counts:\n",
    "        most_frequent_wrong_predictions[gold] = max(class_counts[gold], key=class_counts[gold].get)\n",
    "\n",
    "    return wrong_count, most_frequent_wrong_predictions, gold_label_counts, class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_count, common_wrong_class, gold_count, class_count = find_most_frequent_wrong_predictions_no_o(tags_original, prediction_original)\n",
    "print(\"wrong count looks like:\", wrong_count)\n",
    "print(\"common_wrong_class looks like\", common_wrong_class)\n",
    "print(\"gold_count looks like:\", gold_count)\n",
    "print(\"class_count looks like:\", class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_wrong = {}\n",
    "wrong_fraction = {}\n",
    "  \n",
    "    \n",
    "for gold in gold_count.keys():\n",
    "    wrong_fraction[gold] = wrong_count[gold] / gold_count[gold]\n",
    " \n",
    "\n",
    "wrong_fraction = dict(sorted(wrong_fraction.items(), key=lambda x: x[1], reverse=True))    \n",
    "\n",
    "\n",
    "for gold in gold_count.keys():\n",
    "    inner_dict = class_count[gold]\n",
    "    sorted_items = sorted(inner_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "    max_key, max_value = sorted_items[0]\n",
    "    try:\n",
    "        second_max_key, second_max_value = sorted_items[1]\n",
    "    except IndexError:\n",
    "        second_max_key, second_max_value = 'none', 0\n",
    "    common_wrong[gold] = {max_key: max_value, second_max_key: second_max_value}\n",
    "\n",
    "\n",
    "for gold in wrong_fraction.keys():\n",
    "    print(gold, \" \", gold_count[gold], \" \", list(common_wrong[gold].keys())[0], \" \", list(common_wrong[gold].values())[0], \" \", list(common_wrong[gold].keys())[1], \" \", list(common_wrong[gold].values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_wrong_predictions(gold_labels, predictions):\n",
    "    class_counts = {}  # Dictionary to store counts of wrong predictions for each class\n",
    "    gold_label_counts = {}  # Dictionary to store counts of gold label occurrences\n",
    "    wrong_count = {}\n",
    "\n",
    "    for gold, pred in zip(gold_labels, predictions):\n",
    "        if gold not in gold_label_counts:\n",
    "            gold_label_counts[gold] = 0\n",
    "        gold_label_counts[gold] += 1  # Increment count for gold label occurrence\n",
    "\n",
    "        if gold != pred:  # Check if the prediction is wrong\n",
    "            add_or_increment_key(wrong_count, gold)\n",
    "            if gold not in class_counts:\n",
    "                class_counts[gold] = {}\n",
    "            if pred not in class_counts[gold]:\n",
    "                class_counts[gold][pred] = 0\n",
    "            class_counts[gold][pred] += 1  # Increment count for the predicted class\n",
    "\n",
    "    most_frequent_wrong_predictions = {}\n",
    "    for gold in class_counts:\n",
    "        most_frequent_wrong_predictions[gold] = max(class_counts[gold], key=class_counts[gold].get)\n",
    "\n",
    "    return wrong_count, most_frequent_wrong_predictions, gold_label_counts, class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_count, common_wrong_class, gold_count, class_count = find_most_frequent_wrong_predictions(tags_original, prediction_original)\n",
    "print(\"wrong count looks like:\", wrong_count)\n",
    "print(\"common_wrong_class looks like\", common_wrong_class)\n",
    "print(\"gold_count looks like:\", gold_count)\n",
    "print(\"class_count looks like:\", class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_wrong = {}\n",
    "wrong_fraction = {}\n",
    "  \n",
    "    \n",
    "for gold in gold_count.keys():\n",
    "    wrong_fraction[gold] = wrong_count[gold] / gold_count[gold]\n",
    " \n",
    "\n",
    "wrong_fraction = dict(sorted(wrong_fraction.items(), key=lambda x: x[1], reverse=True))    \n",
    "\n",
    "\n",
    "for gold in gold_count.keys():\n",
    "    inner_dict = class_count[gold]\n",
    "    sorted_items = sorted(inner_dict.items(), key=lambda x: x[1], reverse = True)\n",
    "    max_key, max_value = sorted_items[0]\n",
    "    try:\n",
    "        second_max_key, second_max_value = sorted_items[1]\n",
    "    except IndexError:\n",
    "        second_max_key, second_max_value = 'none', 0\n",
    "    common_wrong[gold] = {max_key: max_value, second_max_key: second_max_value}\n",
    "\n",
    "\n",
    "for gold in wrong_fraction.keys():\n",
    "    print(gold, \" \", gold_count[gold], \" \", list(common_wrong[gold].keys())[0], \" \", list(common_wrong[gold].values())[0], \" \", list(common_wrong[gold].keys())[1], \" \", list(common_wrong[gold].values())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(lst, target):\n",
    "    indexes = []\n",
    "    for i, item in enumerate(lst):\n",
    "        if item == target:\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "def find_predictions(dataframe, label):\n",
    "    for i, row in dataframe.iterrows():\n",
    "        if label in row['tags']:\n",
    "            print(row['sentence'])\n",
    "            print(row['tags'])\n",
    "            try:\n",
    "                print(row['prediction_list'])\n",
    "            except:\n",
    "                print(row['predictions'])\n",
    "            \n",
    "            print('-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_predictions(comp_df, 'OtherPER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_wrong_predictions_no_o(gold_labels, predictions):\n",
    "    class_counts = {}  # Dictionary to store counts of wrong predictions for each class\n",
    "    gold_label_counts = {}  # Dictionary to store counts of gold label occurrences\n",
    "\n",
    "    for gold, pred in zip(gold_labels, predictions):\n",
    "        if gold not in gold_label_counts:\n",
    "            gold_label_counts[gold] = 0\n",
    "        gold_label_counts[gold] += 1  # Increment count for gold label occurrence\n",
    "        \n",
    "        if pred == \"O\":\n",
    "            continue\n",
    "\n",
    "        if gold != pred:  # Check if the prediction is wrong\n",
    "            if gold not in class_counts:\n",
    "                class_counts[gold] = {}\n",
    "            if pred not in class_counts[gold]:\n",
    "                class_counts[gold][pred] = 0\n",
    "            class_counts[gold][pred] += 1  # Increment count for the predicted class\n",
    "\n",
    "    most_frequent_wrong_predictions = {}\n",
    "    for gold in class_counts:\n",
    "        most_frequent_wrong_predictions[gold] = max(class_counts[gold], key=class_counts[gold].get)\n",
    "\n",
    "    return most_frequent_wrong_predictions, gold_label_counts, class_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_count, gold_count, class_count = find_most_frequent_wrong_predictions_no_o(tags_original, prediction_original)\n",
    "print(\"wrong count looks like:\", wrong_count)\n",
    "print(\"gold_count looks like:\", gold_count)\n",
    "print(\"class_count looks like:\", class_count)\n",
    "\n",
    "common_wrong = {}\n",
    "    \n",
    "for gold in gold_count.keys():\n",
    "    try:\n",
    "        inner_dict = class_count[gold]\n",
    "        max_value = max(inner_dict.values())\n",
    "        max_key = max(inner_dict, key=inner_dict.get)\n",
    "        common_wrong[gold] = {max_key: max_value}\n",
    "    except:\n",
    "        common_wrong[gold] = {'none': 0}\n",
    "\n",
    "print(common_wrong)\n",
    "\n",
    "for gold in gold_count.keys():\n",
    "    print(\"Label\", gold, \"occured\", gold_count[gold], \"and was mislabelled as\", list(common_wrong[gold].keys())[0], list(common_wrong[gold].values())[0], \"times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_error(dataframe):\n",
    "    correct_span = []\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        gold_labels = dataframe['tags'][index]\n",
    "        pred_labels = dataframe['prediction_list'][index]\n",
    "        for ind in range(len(gold_labels)):\n",
    "            if gold_labels[ind] != 'O' and pred_labels[ind] == 'O':\n",
    "                correct_span.append(0)\n",
    "                break\n",
    "            if ind + 1 == len(gold_labels):\n",
    "                correct_span.append(1)\n",
    "        \n",
    "    new_df = pd.DataFrame({'sentence': dataframe['sentence'], 'tags': dataframe['tags'], 'prediction_list': dataframe['prediction_list'], 'correct_span': correct_span})\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_error(dataframe):\n",
    "    correct_span = []\n",
    "    number_of_spans = []\n",
    "    number_missed_entity = []\n",
    "    number_wrong_entity = []\n",
    "    number_non_entity = []\n",
    "    \n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        gold_labels = dataframe['tags'][index]\n",
    "        pred_labels = dataframe['prediction_list'][index]\n",
    "        correct_span_sentence = True\n",
    "        span_counter = 0\n",
    "        wrong_entity = 0\n",
    "        non_entity = 0\n",
    "        missed_entity = 0\n",
    "        for ind in range(len(gold_labels)):\n",
    "            if gold_labels[ind] != 'O':\n",
    "                try: \n",
    "                    if gold_labels[ind + 1] == gold_labels[ind]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        span_counter += 1\n",
    "                except:\n",
    "                    span_counter += 1\n",
    "                    continue\n",
    "            \n",
    "            if gold_labels[ind] != pred_labels[ind]:\n",
    "                correct_span_sentence = False\n",
    "                if gold_labels[ind] == 'O':\n",
    "                    non_entity += 1\n",
    "                elif pred_labels[ind] == 'O':\n",
    "                    missed_entity += 1\n",
    "                else:\n",
    "                    wrong_entity += 1\n",
    "                    \n",
    "                    \n",
    "        if not correct_span_sentence:\n",
    "            correct_span.append(0)\n",
    "        else:\n",
    "            correct_span.append(1)\n",
    "        \n",
    "        number_of_spans.append(span_counter)\n",
    "        number_missed_entity.append(missed_entity)\n",
    "        number_wrong_entity.append(wrong_entity)\n",
    "        number_non_entity.append(non_entity)\n",
    "        \n",
    "    new_df = pd.DataFrame({'sentence': dataframe['sentence'], 'tags': dataframe['tags'], 'prediction_list': dataframe['prediction_list'], 'number_of_spans': number_of_spans, 'number_wrong_entities': number_wrong_entity, 'number_non_entities': number_non_entity, 'number_missed_entities': number_missed_entity, 'correct_span': correct_span})\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_df = span_error(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_df[some_df['correct_span'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_values = some_df[some_df['correct_span'] == 0]['number_of_spans'].value_counts()\n",
    "correct_values = some_df[some_df['correct_span'] == 1]['number_of_spans'].value_counts()\n",
    "\n",
    "for span_count in wrong_values.keys():\n",
    "    print('Total of span count', span_count, ':', wrong_values[span_count] + correct_values[span_count])\n",
    "    print(correct_values[span_count], 'correct')\n",
    "    print(wrong_values[span_count], 'wrong')\n",
    "    print('proportion correct is:', correct_values[span_count] / (wrong_values[span_count] + correct_values[span_count]))\n",
    "    print('-' * 25)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_span_wrong = some_df[some_df['correct_span'] == 0]['number_of_spans'].mean()\n",
    "mean_span_correct = some_df[some_df['correct_span'] == 1]['number_of_spans'].mean()\n",
    "\n",
    "print(mean_span_wrong)\n",
    "print(mean_span_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_values = some_df[some_df['correct_span'] == 0]['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "correct_values = some_df[some_df['correct_span'] == 1]['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "print('the average incorrect sentence length is', wrong_values.mean())\n",
    "print('the average correct sentence length is', correct_values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_values = some_df[some_df['correct_span'] == 0]\n",
    "\n",
    "print('for the wrong predicted sentences:')\n",
    "print('number of wrong predictions:', wrong_values['number_wrong_entities'].sum())\n",
    "print('number of false predicted entities:', wrong_values['number_non_entities'].sum())\n",
    "print('number of missed entities:', wrong_values['number_missed_entities'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the text complexity for each sentence\n",
    "def text_complexity(sentence_df):\n",
    "    flesch_scores = []  \n",
    "    correct = []\n",
    "    \n",
    "    for index, row in sentence_df.iterrows():\n",
    "        flesch_score = flesch_reading_ease(row['sentence'])\n",
    "        flesch_scores.append(flesch_score)\n",
    "        if row['tags'] == row['prediction_list']:\n",
    "            correct.append(1)\n",
    "        else:\n",
    "            correct.append(0)\n",
    "\n",
    "    tc_sentence_df = sentence_df.assign(text_complexity = flesch_scores)\n",
    "    tc_sentence_df = tc_sentence_df.assign(correct = correct)\n",
    "    \n",
    "    return tc_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_tc = text_complexity(final_df)\n",
    "final_df_tc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_tc[final_df_tc['correct'] == 0]['text_complexity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_tc[final_df_tc['correct'] == 1]['text_complexity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences using the vectorizer\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(some_df['sentence'])\n",
    "\n",
    "# Calculate the sum of TF-IDF scores for each sentence\n",
    "sentence_tfidf_sum = tfidf_matrix.sum(axis=1)\n",
    "\n",
    "# Create a new column in the dataframe to store the sum of TF-IDF scores\n",
    "some_df['tfidf_sum'] = sentence_tfidf_sum\n",
    "\n",
    "# Print the dataframe with the sum of TF-IDF scores\n",
    "print(some_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(some_df[some_df['correct_span'] == 0]['tfidf_sum'].mean())\n",
    "print(some_df[some_df['correct_span'] == 1]['tfidf_sum'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_wrong_missed(dataframe):\n",
    "    count = 0\n",
    "    wrong = 0\n",
    "    missed = 0\n",
    "    \n",
    "    for index in range(len(dataframe)):\n",
    "        previous_label = ''\n",
    "        wrong_tracker = False\n",
    "        miss_tracker = False\n",
    "        gold_tags = dataframe.iloc[index]['tags']\n",
    "        pred_tags = dataframe.iloc[index]['prediction_list']\n",
    "        for ind in range(len(gold_tags)):\n",
    "            if gold_tags[ind] == 'O':\n",
    "                previous_label = ''\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != previous_label:\n",
    "                wrong_tracker = False\n",
    "                miss_tracker = False\n",
    "                previous_label = gold_tags[ind]\n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != pred_tags[ind] and pred_tags[ind] != 'O':\n",
    "                wrong_tracker = True\n",
    "            if gold_tags[ind] != 'O' and pred_tags[ind] == 'O':\n",
    "                miss_tracker = True\n",
    "                            \n",
    "            try:\n",
    "                next_label = gold_tags[ind + 1]\n",
    "            except:\n",
    "                next_label = ''\n",
    "                \n",
    "            if gold_tags[ind] != 'O' and gold_tags[ind] != next_label:\n",
    "                count += 1\n",
    "            if gold_tags[ind] != next_label and wrong_tracker:\n",
    "                wrong += 1\n",
    "                wrong_tracker = False\n",
    "            if gold_tags[ind] != next_label and miss_tracker:\n",
    "                missed += 1\n",
    "                miss_tracker = False\n",
    "                \n",
    "    print('There are', count, 'entities. \\n', 'The model labeled', wrong, ' wrong. \\n', 'The model missed', missed)\n",
    "\n",
    "\n",
    "number_wrong_missed(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "classification_rep = \"\"\"\n",
    "                         precision    recall  f1-score   support\n",
    "\n",
    "B-AerospaceManufacturer       0.53      0.81      0.64        48\n",
    "  B-AnatomicalStructure       0.86      0.78      0.82       285\n",
    "              B-ArtWork       0.64      0.66      0.65        35\n",
    "               B-Artist       0.74      0.77      0.75      1770\n",
    "              B-Athlete       0.76      0.72      0.74       921\n",
    "      B-CarManufacturer       0.69      0.64      0.66       103\n",
    "               B-Cleric       0.75      0.46      0.57       166\n",
    "             B-Clothing       0.71      0.62      0.67        88\n",
    "              B-Disease       0.91      0.84      0.88       256\n",
    "                B-Drink       0.84      0.44      0.57        71\n",
    "             B-Facility       0.72      0.64      0.68       511\n",
    "                 B-Food       0.77      0.57      0.66       184\n",
    "      B-HumanSettlement       0.92      0.89      0.90      1576\n",
    "     B-MedicalProcedure       0.91      0.76      0.83       199\n",
    "   B-Medication/Vaccine       0.91      0.78      0.84       242\n",
    "           B-MusicalGRP       0.74      0.66      0.70       348\n",
    "          B-MusicalWork       0.82      0.71      0.76       436\n",
    "                  B-ORG       0.68      0.59      0.63       748\n",
    "             B-OtherLOC       0.57      0.42      0.48       191\n",
    "             B-OtherPER       0.46      0.52      0.49       797\n",
    "            B-OtherPROD       0.73      0.61      0.67       409\n",
    "           B-Politician       0.59      0.48      0.53       584\n",
    "          B-PrivateCorp       0.19      0.43      0.26        21\n",
    "           B-PublicCorp       0.64      0.51      0.57       268\n",
    "            B-Scientist       0.44      0.58      0.50       158\n",
    "             B-Software       0.81      0.62      0.71       293\n",
    "            B-SportsGRP       0.88      0.87      0.88       455\n",
    "        B-SportsManager       0.53      0.54      0.54       167\n",
    "              B-Station       0.87      0.80      0.84       205\n",
    "              B-Symptom       0.58      0.53      0.55        53\n",
    "              B-Vehicle       0.66      0.57      0.62       195\n",
    "           B-VisualWork       0.75      0.69      0.72       508\n",
    "          B-WrittenWork       0.79      0.65      0.71       398\n",
    "I-AerospaceManufacturer       0.68      0.75      0.71        28\n",
    "  I-AnatomicalStructure       0.80      0.57      0.67        75\n",
    "              I-ArtWork       0.86      0.48      0.62        77\n",
    "               I-Artist       0.76      0.79      0.77      2038\n",
    "              I-Athlete       0.78      0.75      0.76      1124\n",
    "      I-CarManufacturer       0.41      0.43      0.42        28\n",
    "               I-Cleric       0.65      0.48      0.55       205\n",
    "             I-Clothing       0.40      0.25      0.31        16\n",
    "              I-Disease       0.90      0.77      0.83       126\n",
    "                I-Drink       0.74      0.56      0.64        25\n",
    "             I-Facility       0.76      0.73      0.75       569\n",
    "                 I-Food       0.85      0.53      0.65        62\n",
    "      I-HumanSettlement       0.91      0.89      0.90       577\n",
    "     I-MedicalProcedure       0.88      0.59      0.71       108\n",
    "   I-Medication/Vaccine       0.92      0.55      0.69        40\n",
    "           I-MusicalGRP       0.81      0.68      0.74       316\n",
    "          I-MusicalWork       0.86      0.78      0.82       656\n",
    "                  I-ORG       0.74      0.68      0.71       883\n",
    "             I-OtherLOC       0.73      0.59      0.65       275\n",
    "             I-OtherPER       0.45      0.57      0.50      1047\n",
    "            I-OtherPROD       0.70      0.52      0.60       204\n",
    "           I-Politician       0.59      0.47      0.52       733\n",
    "          I-PrivateCorp       0.32      0.50      0.39        18\n",
    "           I-PublicCorp       0.58      0.38      0.46       129\n",
    "            I-Scientist       0.46      0.61      0.53       204\n",
    "             I-Software       0.87      0.61      0.72       258\n",
    "            I-SportsGRP       0.93      0.85      0.89       567\n",
    "        I-SportsManager       0.56      0.55      0.55       197\n",
    "              I-Station       0.89      0.85      0.87       205\n",
    "              I-Symptom       0.47      0.33      0.39        27\n",
    "              I-Vehicle       0.72      0.53      0.61       176\n",
    "           I-VisualWork       0.78      0.71      0.74       740\n",
    "          I-WrittenWork       0.87      0.68      0.76       554\n",
    "                      O       0.00      1.00      0.00         0\n",
    "\"\"\"\n",
    "\n",
    "# Extracting f1-scores and support counts from the classification report\n",
    "lines = classification_rep.split('\\n')\n",
    "data = lines[2:-5]  # Extract relevant lines\n",
    "\n",
    "f1_scores = []\n",
    "supports = []\n",
    "\n",
    "for line in data:\n",
    "    tokens = line.split()\n",
    "    if len(tokens) > 4:\n",
    "        if tokens[0] != 'O':\n",
    "            f1_scores.append(float(tokens[3]))\n",
    "            supports.append(int(tokens[4]))\n",
    "\n",
    "# Creating a DataFrame for the data\n",
    "df = pd.DataFrame({'Support Count': supports, 'F1-Score': f1_scores})\n",
    "\n",
    "# Calculating the correlation coefficient and p-value\n",
    "correlation, p_value = stats.pearsonr(df['Support Count'], df['F1-Score'])\n",
    "\n",
    "# Creating the scatter plot using Seaborn\n",
    "sns.scatterplot(data=df, x='Support Count', y='F1-Score')\n",
    "plt.xlabel('Support Count')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score vs Support Count')\n",
    "plt.text(0.4, 0.01, f'Correlation: {correlation:.2f} (p-value: {p_value:.4f})', transform=plt.gca().transAxes, fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:master_thesis]",
   "language": "python",
   "name": "conda-env-master_thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
